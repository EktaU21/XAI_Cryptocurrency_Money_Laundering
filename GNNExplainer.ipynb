{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EktaU21/XAI_Cryptocurrency_Money_Laundering/blob/main/GNNExplainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "X8cL0ZqBvs1z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27f93b29-78fd-44d0-aa2b-5891ce77c37f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1lAxt1pvyEF"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTw5w4psv-K8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQpU67VNwWMJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_wuNIt-HwEmj"
      },
      "outputs": [],
      "source": [
        "import os.path as osp\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from torch_geometric.explain import Explainer, GNNExplainer\n",
        "from torch_geometric.nn import GCNConv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7_oodQ7IPtXV"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import ChebConv, NNConv, DeepGCNLayer, GATConv, DenseGCNConv, GCNConv, GraphConv\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm, trange"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GNNExplainer_GAT"
      ],
      "metadata": {
        "id": "qLst8EMFmvqK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdWxy4XJn2Db"
      },
      "outputs": [],
      "source": [
        "def test(srno):\n",
        "\n",
        "  df_features = pd.read_csv('/content/drive/MyDrive/elliptic_bitcoin_dataset/elliptic_txs_features.csv', header=None)\n",
        "  df_edges = pd.read_csv('/content/drive/MyDrive/elliptic_bitcoin_dataset/elliptic_txs_edgelist.csv')\n",
        "  df_classes =  pd.read_csv('/content/drive/MyDrive/elliptic_bitcoin_dataset/elliptic_txs_classes.csv')\n",
        "  df_classes['class'] = df_classes['class'].map({'unknown': 2, '1':1, '2':0})\n",
        "\n",
        "  # merging dataframes\n",
        "  df_merge = df_features.merge(df_classes, how='left', right_on=\"txId\", left_on=0)\n",
        "  df_merge = df_merge.loc[df_merge[1] == srno]\n",
        "  df_merge = df_merge.sort_values(0).reset_index(drop=True)\n",
        "  classified = df_merge.loc[df_merge['class'].loc[df_merge['class']!=2].index].drop('txId', axis=1)\n",
        "  unclassified = df_merge.loc[df_merge['class'].loc[df_merge['class']==2].index].drop('txId', axis=1)\n",
        "\n",
        "  # storing classified unclassified nodes seperatly for training and testing purpose\n",
        "  classified_edges = df_edges.loc[df_edges['txId1'].isin(classified[0]) & df_edges['txId2'].isin(classified[0])]\n",
        "  unclassified_edges = df_edges.loc[df_edges['txId1'].isin(unclassified[0]) | df_edges['txId2'].isin(unclassified[0])]\n",
        "  del df_features, df_classes\n",
        "\n",
        "  edgess =  df_edges.loc[df_edges['txId1'].isin(df_merge[0]) & df_edges['txId2'].isin(df_merge[0])]\n",
        "\n",
        "  # all nodes in data\n",
        "  nodes = df_merge[0].values\n",
        "  map_id = {j:i for i,j in enumerate(nodes)} # mapping nodes to indexes\n",
        "\n",
        "  edges = edgess.copy()\n",
        "  edges.txId1 = edges.txId1.map(map_id)\n",
        "  edges.txId2 = edges.txId2.map(map_id)\n",
        "\n",
        "  edge_index = np.array(edges.values).T\n",
        "\n",
        "  edge_index = torch.tensor(edge_index, dtype=torch.long).contiguous()\n",
        "  weights = torch.tensor([1]* edge_index.shape[1] , dtype=torch.double)\n",
        "\n",
        "  # maping txIds to corresponding indexes, to pass node features to the model\n",
        "  node_features = df_merge.drop(['txId'], axis=1).copy()\n",
        "  node_features[0] = node_features[0].map(map_id)\n",
        "\n",
        "  illicit_idx = node_features['class'].loc[node_features['class']==1].index\n",
        "  print(len(illicit_idx))\n",
        "\n",
        "  # replace unkown class with 0, to avoid having 3 classes, this data/labels never used in training\n",
        "  node_features['class'] = node_features['class'].replace(2, 0)\n",
        "\n",
        "  node_idx = node_features['class'].index\n",
        "\n",
        "  labels = node_features['class'].values\n",
        "  node_features = torch.tensor(np.array(node_features.drop([0, 'class', 1], axis=1).values, dtype=np.double), dtype=torch.double)\n",
        "\n",
        "  # converting data to PyGeometric graph data format\n",
        "  data_train = Data(x=node_features, edge_index=edge_index, edge_attr=weights,\n",
        "                               y=torch.tensor(labels, dtype=torch.double)) #, adj= torch.from_numpy(np.array(adj))\n",
        "\n",
        "  y_train = labels[node_idx]\n",
        "\n",
        "  # spliting train set and validation set\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  X_train, X_valid, y_train, y_valid, train_idx, valid_idx = train_test_split(node_features[node_idx], y_train, node_idx, test_size=0.15, random_state=42, stratify=y_train)\n",
        "\n",
        "  train_idx\n",
        "  ten_train_idx = torch.tensor(train_idx)\n",
        "  ten_train_idx\n",
        "\n",
        "  from torch_geometric.utils import index_to_mask, mask_to_index\n",
        "  train_mask = index_to_mask(ten_train_idx, size= len(labels))\n",
        "  train_mask.shape\n",
        "\n",
        "  valid_idx\n",
        "  ten_valid_idx = torch.tensor(valid_idx)\n",
        "  ten_valid_idx\n",
        "\n",
        "  from torch_geometric.utils import index_to_mask, mask_to_index\n",
        "  val_mask = index_to_mask(ten_valid_idx, size= len(labels))\n",
        "  val_mask.shape\n",
        "\n",
        "  # converting data to PyGeometric graph data format\n",
        "  data_train = Data(x=torch.tensor(node_features,dtype=torch.float),\n",
        "                  edge_index=edge_index, #edge_attr=weights,\n",
        "                               y=torch.tensor(labels, dtype=torch.long),\n",
        "                  train_mask=torch.tensor(train_mask,dtype=torch.bool),\n",
        "                  val_mask=torch.tensor(val_mask,dtype=torch.bool)) #, adj= torch.from_numpy(np.array(adj))\n",
        "\n",
        "  print(data_train)\n",
        "\n",
        "  torch.manual_seed(42)\n",
        "  #define a GAT class\n",
        "\n",
        "  out =[]\n",
        "  for j in range(len(illicit_idx)):\n",
        "    node_idx = illicit_idx[j]\n",
        "    out.append(node_idx)\n",
        "  print(out)\n",
        "\n",
        "  def demo(idx):\n",
        "\n",
        "    class GAT(nn.Module):\n",
        "\n",
        "      def __init__(self, input_dim, hidden_dim, output_dim,\n",
        "                 heads_1=8, heads_2=1, att_dropout=0.6, input_dropout=0.6):\n",
        "\n",
        "          super(GAT, self).__init__()\n",
        "\n",
        "          self.att_dropout = att_dropout\n",
        "          self.input_dropout = input_dropout\n",
        "\n",
        "          self.conv1 = GATConv(in_channels=input_dim,\n",
        "                             out_channels=hidden_dim // heads_1,\n",
        "                             heads=heads_1,\n",
        "                             concat=True,\n",
        "                             dropout=att_dropout)\n",
        "          self.conv2 = GATConv(in_channels=hidden_dim,\n",
        "                             out_channels=output_dim,\n",
        "                             heads=heads_2,\n",
        "                             concat=False,\n",
        "                             dropout=att_dropout)\n",
        "\n",
        "      def forward(self, x, edge_index):\n",
        "          x = F.dropout(x, p=self.input_dropout, training=self.training)\n",
        "          x = self.conv1(x, edge_index)\n",
        "          x = F.elu(x)\n",
        "          x = F.dropout(x, p=self.input_dropout, training=self.training)\n",
        "          x = self.conv2(x, edge_index)\n",
        "\n",
        "          return F.log_softmax(x, dim=1)\n",
        "\n",
        " #Instantiate a GAT model\n",
        "    hparams = {\n",
        "      'input_dim': data_train.num_node_features,\n",
        "      'hidden_dim':8,\n",
        "      'output_dim': int(max(data_train.y).item()) + 1\n",
        "    }\n",
        "    model = GAT(**hparams)\n",
        "    model\n",
        "\n",
        "  #Train the model\n",
        "    def accuracy(output, labels):\n",
        "      _, pred = output.max(dim=1)\n",
        "      correct = pred.eq(labels).double()\n",
        "      correct = correct.sum()\n",
        "\n",
        "      return correct / len(labels)\n",
        "\n",
        "    lr = 0.005\n",
        "    epochs =300\n",
        "\n",
        "    model.train()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      output = model(data_train.x, data_train.edge_index)\n",
        "      loss = F.nll_loss(output[data_train.train_mask], data_train.y[data_train.train_mask])\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if epoch % 10 == 0:\n",
        "          acc = accuracy(output[data_train.train_mask], data_train.y[data_train.train_mask])\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "  #Compute the coefficients of features\n",
        "\n",
        "    explainer = Explainer(\n",
        "    model=model,\n",
        "    algorithm=GNNExplainer(epochs=300),\n",
        "    explanation_type='model',\n",
        "    node_mask_type='attributes',\n",
        "    edge_mask_type='object',\n",
        "    model_config=dict(\n",
        "        mode='multiclass_classification',\n",
        "        task_level='node',\n",
        "        return_type='log_probs',\n",
        "    ),\n",
        ")\n",
        "\n",
        "    node_index = out[k]\n",
        "    print(\"node index\",node_index)\n",
        "    explanation = explainer(data_train.x, data_train.edge_index, index=node_index)\n",
        "    print(f'Generated explanations in {explanation.available_explanations}')\n",
        "\n",
        "    path = '/content/drive/MyDrive/GNNExplainer/TS_1/GAT/TS22_GAT/feature_importance_'+str(out[k])+'.png'\n",
        "    explanation.visualize_feature_importance(path, top_k=5)\n",
        "    print(f\"Feature importance plot has been saved to '{path}'\")\n",
        "\n",
        "    path = '/content/drive/MyDrive/GNNExplainer/TS_1/GAT/TS22_GAT/subgraph_'+str(out[k])+'.png'\n",
        "    explanation.visualize_graph(path)\n",
        "\n",
        "\n",
        "  for k in range(len(out)):\n",
        "      print(\"for id\"+str(out[k])+\":\")\n",
        "      demo(out[k])\n",
        "      print(\"call demo for:\",out[k])\n",
        "\n",
        "#test() is called by passing the desired time step number as an argument.\n",
        "test(1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GNNExplainer_GAT_All"
      ],
      "metadata": {
        "id": "MIT8Ra9N232r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(srno):\n",
        "\n",
        "  df_features = pd.read_csv('/content/drive/MyDrive/elliptic_bitcoin_dataset/elliptic_txs_features.csv', header=None)\n",
        "  df_edges = pd.read_csv('/content/drive/MyDrive/elliptic_bitcoin_dataset/elliptic_txs_edgelist.csv')\n",
        "  df_classes =  pd.read_csv('/content/drive/MyDrive/elliptic_bitcoin_dataset/elliptic_txs_classes.csv')\n",
        "  df_classes['class'] = df_classes['class'].map({'unknown': 2, '1':1, '2':0})\n",
        "\n",
        "  # merging dataframes\n",
        "  df_merge = df_features.merge(df_classes, how='left', right_on=\"txId\", left_on=0)\n",
        "  df_merge = df_merge.loc[df_merge[1] == srno]\n",
        "  df_merge = df_merge.sort_values(0).reset_index(drop=True)\n",
        "  classified = df_merge.loc[df_merge['class'].loc[df_merge['class']!=2].index].drop('txId', axis=1)\n",
        "  unclassified = df_merge.loc[df_merge['class'].loc[df_merge['class']==2].index].drop('txId', axis=1)\n",
        "\n",
        "  # storing classified unclassified nodes seperatly for training and testing purpose\n",
        "  classified_edges = df_edges.loc[df_edges['txId1'].isin(classified[0]) & df_edges['txId2'].isin(classified[0])]\n",
        "  unclassified_edges = df_edges.loc[df_edges['txId1'].isin(unclassified[0]) | df_edges['txId2'].isin(unclassified[0])]\n",
        "  del df_features, df_classes\n",
        "\n",
        "  edgess =  df_edges.loc[df_edges['txId1'].isin(df_merge[0]) & df_edges['txId2'].isin(df_merge[0])]\n",
        "\n",
        "  # all nodes in data\n",
        "  nodes = df_merge[0].values\n",
        "  map_id = {j:i for i,j in enumerate(nodes)} # mapping nodes to indexes\n",
        "\n",
        "  edges = edgess.copy()\n",
        "  edges.txId1 = edges.txId1.map(map_id)\n",
        "  edges.txId2 = edges.txId2.map(map_id)\n",
        "\n",
        "  edge_index = np.array(edges.values).T\n",
        "\n",
        "  edge_index = torch.tensor(edge_index, dtype=torch.long).contiguous()\n",
        "  weights = torch.tensor([1]* edge_index.shape[1] , dtype=torch.double)\n",
        "\n",
        "  # maping txIds to corresponding indexes, to pass node features to the model\n",
        "  node_features = df_merge.drop(['txId'], axis=1).copy()\n",
        "  node_features[0] = node_features[0].map(map_id)\n",
        "\n",
        "  illicit_idx = node_features['class'].loc[node_features['class']==1].index\n",
        "  print(len(illicit_idx))\n",
        "\n",
        "  # replace unkown class with 0, to avoid having 3 classes, this data/labels never used in training\n",
        "  node_features['class'] = node_features['class'].replace(2, 0)\n",
        "\n",
        "  node_idx = node_features['class'].index\n",
        "\n",
        "  labels = node_features['class'].values\n",
        "  node_features = torch.tensor(np.array(node_features.drop([0, 'class', 1], axis=1).values, dtype=np.double), dtype=torch.double)\n",
        "\n",
        "  # converting data to PyGeometric graph data format\n",
        "  data_train = Data(x=node_features, edge_index=edge_index, edge_attr=weights,\n",
        "                               y=torch.tensor(labels, dtype=torch.double)) #, adj= torch.from_numpy(np.array(adj))\n",
        "\n",
        "  y_train = labels[node_idx]\n",
        "\n",
        "  # spliting train set and validation set\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  X_train, X_valid, y_train, y_valid, train_idx, valid_idx = train_test_split(node_features[node_idx], y_train, node_idx, test_size=0.15, random_state=42, stratify=y_train)\n",
        "\n",
        "  train_idx\n",
        "  ten_train_idx = torch.tensor(train_idx)\n",
        "  ten_train_idx\n",
        "\n",
        "  from torch_geometric.utils import index_to_mask, mask_to_index\n",
        "  train_mask = index_to_mask(ten_train_idx, size= len(labels))\n",
        "  train_mask.shape\n",
        "\n",
        "  valid_idx\n",
        "  ten_valid_idx = torch.tensor(valid_idx)\n",
        "  ten_valid_idx\n",
        "\n",
        "  from torch_geometric.utils import index_to_mask, mask_to_index\n",
        "  val_mask = index_to_mask(ten_valid_idx, size= len(labels))\n",
        "  val_mask.shape\n",
        "\n",
        "  # converting data to PyGeometric graph data format\n",
        "  data_train = Data(x=torch.tensor(node_features,dtype=torch.float),\n",
        "                  edge_index=edge_index, #edge_attr=weights,\n",
        "                               y=torch.tensor(labels, dtype=torch.long),\n",
        "                  train_mask=torch.tensor(train_mask,dtype=torch.bool),\n",
        "                  val_mask=torch.tensor(val_mask,dtype=torch.bool)) #, adj= torch.from_numpy(np.array(adj))\n",
        "\n",
        "  print(data_train)\n",
        "\n",
        "  torch.manual_seed(42)\n",
        "  #define a GAT class\n",
        "\n",
        "  out =[]\n",
        "  for j in range(len(illicit_idx)):\n",
        "    node_idx = illicit_idx[j]\n",
        "    out.append(node_idx)\n",
        "  print(out)\n",
        "\n",
        "  #def demo(idx):\n",
        "\n",
        "  class GAT(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim,\n",
        "                 heads_1=8, heads_2=1, att_dropout=0.6, input_dropout=0.6):\n",
        "\n",
        "          super(GAT, self).__init__()\n",
        "\n",
        "          self.att_dropout = att_dropout\n",
        "          self.input_dropout = input_dropout\n",
        "\n",
        "          self.conv1 = GATConv(in_channels=input_dim,\n",
        "                             out_channels=hidden_dim // heads_1,\n",
        "                             heads=heads_1,\n",
        "                             concat=True,\n",
        "                             dropout=att_dropout)\n",
        "          self.conv2 = GATConv(in_channels=hidden_dim,\n",
        "                             out_channels=output_dim,\n",
        "                             heads=heads_2,\n",
        "                             concat=False,\n",
        "                             dropout=att_dropout)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "          x = F.dropout(x, p=self.input_dropout, training=self.training)\n",
        "          x = self.conv1(x, edge_index)\n",
        "          x = F.elu(x)\n",
        "          x = F.dropout(x, p=self.input_dropout, training=self.training)\n",
        "          x = self.conv2(x, edge_index)\n",
        "\n",
        "          return F.log_softmax(x, dim=1)\n",
        "\n",
        " #Instantiate a GAT model\n",
        "  hparams = {\n",
        "      'input_dim': data_train.num_node_features,\n",
        "      'hidden_dim':8,\n",
        "      'output_dim': int(max(data_train.y).item()) + 1\n",
        "  }\n",
        "  model = GAT(**hparams)\n",
        "  model\n",
        "\n",
        "  #Train the model\n",
        "  def accuracy(output, labels):\n",
        "      _, pred = output.max(dim=1)\n",
        "      correct = pred.eq(labels).double()\n",
        "      correct = correct.sum()\n",
        "\n",
        "      return correct / len(labels)\n",
        "\n",
        "  lr = 0.005\n",
        "  epochs =300\n",
        "\n",
        "  model.train()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      output = model(data_train.x, data_train.edge_index)\n",
        "      loss = F.nll_loss(output[data_train.train_mask], data_train.y[data_train.train_mask])\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if epoch % 10 == 0:\n",
        "          acc = accuracy(output[data_train.train_mask], data_train.y[data_train.train_mask])\n",
        "        #print('Epoch: {:3d}, acc = {:.3f}'.format(epoch, acc))\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  #Compute the coefficients of features\n",
        "\n",
        "  explainer = Explainer(\n",
        "    model=model,\n",
        "    algorithm=GNNExplainer(epochs=300),\n",
        "    explanation_type='model',\n",
        "    node_mask_type='attributes',\n",
        "    edge_mask_type='object',\n",
        "    model_config=dict(\n",
        "        mode='multiclass_classification',\n",
        "        task_level='node',\n",
        "        return_type='log_probs',\n",
        "    ),\n",
        ")\n",
        "\n",
        "  node_index = out\n",
        "  print(\"node index\",node_index)\n",
        "  explanation = explainer(data_train.x, data_train.edge_index, index=node_index)\n",
        "  print(\"index is:\", explanation.index)\n",
        "  print(f'Generated explanations in {explanation.available_explanations}')\n",
        "\n",
        "  path = '/content/drive/MyDrive/GNNExplainer/TS_1/GAT/TS6_GAT/feature_importance_all.png'\n",
        "  explanation.visualize_feature_importance(path, top_k=5)\n",
        "  #print(f\"Feature importance plot has been saved to '{path}'\")\n",
        "\n",
        "  path = '/content/drive/MyDrive/GNNExplainer/TS_1/GAT/subgraph_'+str(out[k])+'.png'\n",
        "  explanation.visualize_graph(path)\n",
        "\n",
        "\n",
        "#test() is called by passing the desired time step number as an argument.\n",
        "test(6)"
      ],
      "metadata": {
        "id": "tz05Je5Y3BB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# GNNExplainer_GCN"
      ],
      "metadata": {
        "id": "l5HYcSQ-mMRn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWdjhbl0-VoY"
      },
      "outputs": [],
      "source": [
        "def test(srno):\n",
        "\n",
        "  df_features = pd.read_csv('/content/drive/MyDrive/elliptic_bitcoin_dataset/elliptic_txs_features.csv', header=None)\n",
        "  df_edges = pd.read_csv('/content/drive/MyDrive/elliptic_bitcoin_dataset/elliptic_txs_edgelist.csv')\n",
        "  df_classes =  pd.read_csv('/content/drive/MyDrive/elliptic_bitcoin_dataset/elliptic_txs_classes.csv')\n",
        "  df_classes['class'] = df_classes['class'].map({'unknown': 2, '1':1, '2':0})\n",
        "\n",
        "  # merging dataframes\n",
        "  df_merge = df_features.merge(df_classes, how='left', right_on=\"txId\", left_on=0)\n",
        "  df_merge = df_merge.loc[df_merge[1] == srno]\n",
        "  df_merge = df_merge.sort_values(0).reset_index(drop=True)\n",
        "  classified = df_merge.loc[df_merge['class'].loc[df_merge['class']!=2].index].drop('txId', axis=1)\n",
        "  unclassified = df_merge.loc[df_merge['class'].loc[df_merge['class']==2].index].drop('txId', axis=1)\n",
        "\n",
        "  # storing classified unclassified nodes seperatly for training and testing purpose\n",
        "  classified_edges = df_edges.loc[df_edges['txId1'].isin(classified[0]) & df_edges['txId2'].isin(classified[0])]\n",
        "  unclassified_edges = df_edges.loc[df_edges['txId1'].isin(unclassified[0]) | df_edges['txId2'].isin(unclassified[0])]\n",
        "  del df_features, df_classes\n",
        "\n",
        "  edgess =  df_edges.loc[df_edges['txId1'].isin(df_merge[0]) & df_edges['txId2'].isin(df_merge[0])]\n",
        "\n",
        "  # all nodes in data\n",
        "  nodes = df_merge[0].values\n",
        "  map_id = {j:i for i,j in enumerate(nodes)} # mapping nodes to indexes\n",
        "\n",
        "  edges = edgess.copy()\n",
        "  edges.txId1 = edges.txId1.map(map_id)\n",
        "  edges.txId2 = edges.txId2.map(map_id)\n",
        "\n",
        "  edge_index = np.array(edges.values).T\n",
        "\n",
        "  edge_index = torch.tensor(edge_index, dtype=torch.long).contiguous()\n",
        "  weights = torch.tensor([1]* edge_index.shape[1] , dtype=torch.double)\n",
        "\n",
        "  # maping txIds to corresponding indexes, to pass node features to the model\n",
        "  node_features = df_merge.drop(['txId'], axis=1).copy()\n",
        "  node_features[0] = node_features[0].map(map_id)\n",
        "\n",
        "  illicit_idx = node_features['class'].loc[node_features['class']==1].index\n",
        "  print(len(illicit_idx))\n",
        "\n",
        "  # replace unkown class with 0, to avoid having 3 classes, this data/labels never used in training\n",
        "  node_features['class'] = node_features['class'].replace(2, 0)\n",
        "\n",
        "  node_idx = node_features['class'].index\n",
        "\n",
        "  labels = node_features['class'].values\n",
        "  node_features = torch.tensor(np.array(node_features.drop([0, 'class', 1], axis=1).values, dtype=np.double), dtype=torch.double)\n",
        "\n",
        "  # converting data to PyGeometric graph data format\n",
        "  data_train = Data(x=node_features, edge_index=edge_index, edge_attr=weights,\n",
        "                               y=torch.tensor(labels, dtype=torch.double)) #, adj= torch.from_numpy(np.array(adj))\n",
        "\n",
        "  y_train = labels[node_idx]\n",
        "\n",
        "  # spliting train set and validation set\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  X_train, X_valid, y_train, y_valid, train_idx, valid_idx = train_test_split(node_features[node_idx], y_train, node_idx, test_size=0.15, random_state=42, stratify=y_train)\n",
        "\n",
        "  train_idx\n",
        "  ten_train_idx = torch.tensor(train_idx)\n",
        "  ten_train_idx\n",
        "\n",
        "  from torch_geometric.utils import index_to_mask, mask_to_index\n",
        "  train_mask = index_to_mask(ten_train_idx, size= len(labels))\n",
        "  train_mask.shape\n",
        "\n",
        "  valid_idx\n",
        "  ten_valid_idx = torch.tensor(valid_idx)\n",
        "  ten_valid_idx\n",
        "\n",
        "  from torch_geometric.utils import index_to_mask, mask_to_index\n",
        "  val_mask = index_to_mask(ten_valid_idx, size= len(labels))\n",
        "  val_mask.shape\n",
        "\n",
        "  # converting data to PyGeometric graph data format\n",
        "  data_train = Data(x=torch.tensor(node_features,dtype=torch.float),\n",
        "                  edge_index=edge_index, #edge_attr=weights,\n",
        "                               y=torch.tensor(labels, dtype=torch.long),\n",
        "                  train_mask=torch.tensor(train_mask,dtype=torch.bool),\n",
        "                  val_mask=torch.tensor(val_mask,dtype=torch.bool)) #, adj= torch.from_numpy(np.array(adj))\n",
        "\n",
        "  torch.manual_seed(42)\n",
        "\n",
        "  out =[]\n",
        "  for j in range(len(illicit_idx)):\n",
        "    node_idx = illicit_idx[j]\n",
        "    out.append(node_idx)\n",
        "  print(out)\n",
        "\n",
        "  def demo(idx):\n",
        "\n",
        "    #define a GCN class\n",
        "\n",
        "    class Net(torch.nn.Module):\n",
        "          def __init__(self, num_features, dim=16, num_classes=1):\n",
        "            super(Net, self).__init__()\n",
        "            self.conv1 = GCNConv(num_features, dim)\n",
        "            self.conv2 = GCNConv(dim, num_classes)\n",
        "\n",
        "          def forward(self, x, edge_index, data=None):\n",
        "            x = F.relu(self.conv1(x, edge_index))\n",
        "            x = F.dropout(x, training=self.training)\n",
        "            x = self.conv2(x, edge_index)\n",
        "            return F.log_softmax(x, dim=1)\n",
        "\n",
        "    epochs = 300\n",
        "    dim = 16\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = Net(num_features=data_train.num_node_features, dim=dim, num_classes=int(max(data_train.y).item())+1).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-3)\n",
        "\n",
        "    def test(model, data):\n",
        "        model.eval()\n",
        "        logits, accs = model(data_train.x, data_train.edge_index, data), []\n",
        "        for _, mask in data('train_mask', 'val_mask'):\n",
        "            pred = logits[mask].max(1)[1]\n",
        "            acc = pred.eq(data_train.y[mask]).sum().item() / mask.sum().item()\n",
        "            accs.append(acc)\n",
        "        return accs\n",
        "\n",
        "\n",
        "    loss = 999.0\n",
        "    train_acc = 0.0\n",
        "    test_acc = 0.0\n",
        "\n",
        "    t = trange(epochs, desc=\"Stats: \", position=0)\n",
        "\n",
        "    for epoch in t:\n",
        "\n",
        "      model.train()\n",
        "\n",
        "      loss = 0\n",
        "\n",
        "      data = data_train.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      log_logits = model(data_train.x, data_train.edge_index, data_train)\n",
        "\n",
        "      # Since the data is a single huge graph, training on the training set is done by masking the nodes that are not in the training set.\n",
        "      loss = F.nll_loss(log_logits[data_train.train_mask], data_train.y[data_train.train_mask])\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # validate\n",
        "      train_acc, test_acc = test(model, data_train)\n",
        "      train_loss = loss\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    #Compute the coefficients of features\n",
        "    explainer = Explainer(\n",
        "      model=model,\n",
        "      algorithm=GNNExplainer(epochs=300),\n",
        "      explanation_type='model',\n",
        "      node_mask_type='attributes',\n",
        "      edge_mask_type='object',\n",
        "      model_config=dict(\n",
        "          mode='multiclass_classification',\n",
        "          task_level='node',\n",
        "          return_type='log_probs',\n",
        "    ),\n",
        "  )\n",
        "\n",
        "    node_index = illicit_idx\n",
        "    explanation = explainer(data.x, data.edge_index, index=node_index)\n",
        "    print(f'Generated explanations in {explanation.available_explanations}')\n",
        "\n",
        "    path = '/content/drive/MyDrive/GNNExplainer/TS_1/GCN/TS49_GCN/feature_importance_'+str(out[k])+'.png'\n",
        "    explanation.visualize_feature_importance(path, top_k=5)\n",
        "    print(f\"Feature importance plot has been saved to '{path}'\")\n",
        "\n",
        "    path = '/content/drive/MyDrive/GNNExplainer/TS_1/GCN/subgraph_'+str(out[k])+'.png'\n",
        "    explanation.visualize_graph(path)\n",
        "\n",
        "\n",
        "  for k in range(len(out)):\n",
        "      print(\"for id\"+str(out[k])+\":\")\n",
        "      demo(out)\n",
        "\n",
        "#test() is called by passing the desired time step number as an argument.\n",
        "test(49)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GNNExplainer_GCN_All"
      ],
      "metadata": {
        "id": "-cuzcKRU3ObA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(srno):\n",
        "\n",
        "  df_features = pd.read_csv('/content/drive/MyDrive/elliptic_bitcoin_dataset/elliptic_txs_features.csv', header=None)\n",
        "  df_edges = pd.read_csv('/content/drive/MyDrive/elliptic_bitcoin_dataset/elliptic_txs_edgelist.csv')\n",
        "  df_classes =  pd.read_csv('/content/drive/MyDrive/elliptic_bitcoin_dataset/elliptic_txs_classes.csv')\n",
        "  df_classes['class'] = df_classes['class'].map({'unknown': 2, '1':1, '2':0})\n",
        "\n",
        "  # merging dataframes\n",
        "  df_merge = df_features.merge(df_classes, how='left', right_on=\"txId\", left_on=0)\n",
        "  df_merge = df_merge.loc[df_merge[1] == srno]\n",
        "  df_merge = df_merge.sort_values(0).reset_index(drop=True)\n",
        "  classified = df_merge.loc[df_merge['class'].loc[df_merge['class']!=2].index].drop('txId', axis=1)\n",
        "  unclassified = df_merge.loc[df_merge['class'].loc[df_merge['class']==2].index].drop('txId', axis=1)\n",
        "\n",
        "  # storing classified unclassified nodes seperatly for training and testing purpose\n",
        "  classified_edges = df_edges.loc[df_edges['txId1'].isin(classified[0]) & df_edges['txId2'].isin(classified[0])]\n",
        "  unclassified_edges = df_edges.loc[df_edges['txId1'].isin(unclassified[0]) | df_edges['txId2'].isin(unclassified[0])]\n",
        "  del df_features, df_classes\n",
        "\n",
        "  edgess =  df_edges.loc[df_edges['txId1'].isin(df_merge[0]) & df_edges['txId2'].isin(df_merge[0])]\n",
        "\n",
        "  # all nodes in data\n",
        "  nodes = df_merge[0].values\n",
        "  map_id = {j:i for i,j in enumerate(nodes)} # mapping nodes to indexes\n",
        "\n",
        "  edges = edgess.copy()\n",
        "  edges.txId1 = edges.txId1.map(map_id)\n",
        "  edges.txId2 = edges.txId2.map(map_id)\n",
        "\n",
        "  edge_index = np.array(edges.values).T\n",
        "\n",
        "  edge_index = torch.tensor(edge_index, dtype=torch.long).contiguous()\n",
        "  weights = torch.tensor([1]* edge_index.shape[1] , dtype=torch.double)\n",
        "\n",
        "  # maping txIds to corresponding indexes, to pass node features to the model\n",
        "  node_features = df_merge.drop(['txId'], axis=1).copy()\n",
        "  node_features[0] = node_features[0].map(map_id)\n",
        "\n",
        "  illicit_idx = node_features['class'].loc[node_features['class']==1].index\n",
        "  print(len(illicit_idx))\n",
        "\n",
        "  # replace unkown class with 0, to avoid having 3 classes, this data/labels never used in training\n",
        "  node_features['class'] = node_features['class'].replace(2, 0)\n",
        "\n",
        "  node_idx = node_features['class'].index\n",
        "\n",
        "  labels = node_features['class'].values\n",
        "  node_features = torch.tensor(np.array(node_features.drop([0, 'class', 1], axis=1).values, dtype=np.double), dtype=torch.double)\n",
        "\n",
        "  # converting data to PyGeometric graph data format\n",
        "  data_train = Data(x=node_features, edge_index=edge_index, edge_attr=weights,\n",
        "                               y=torch.tensor(labels, dtype=torch.double)) #, adj= torch.from_numpy(np.array(adj))\n",
        "\n",
        "  y_train = labels[node_idx]\n",
        "\n",
        "  # spliting train set and validation set\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  X_train, X_valid, y_train, y_valid, train_idx, valid_idx = train_test_split(node_features[node_idx], y_train, node_idx, test_size=0.15, random_state=42, stratify=y_train)\n",
        "\n",
        "  train_idx\n",
        "  ten_train_idx = torch.tensor(train_idx)\n",
        "  ten_train_idx\n",
        "\n",
        "  from torch_geometric.utils import index_to_mask, mask_to_index\n",
        "  train_mask = index_to_mask(ten_train_idx, size= len(labels))\n",
        "  train_mask.shape\n",
        "\n",
        "  valid_idx\n",
        "  ten_valid_idx = torch.tensor(valid_idx)\n",
        "  ten_valid_idx\n",
        "\n",
        "  from torch_geometric.utils import index_to_mask, mask_to_index\n",
        "  val_mask = index_to_mask(ten_valid_idx, size= len(labels))\n",
        "  val_mask.shape\n",
        "\n",
        "  # converting data to PyGeometric graph data format\n",
        "  data_train = Data(x=torch.tensor(node_features,dtype=torch.float),\n",
        "                  edge_index=edge_index, #edge_attr=weights,\n",
        "                               y=torch.tensor(labels, dtype=torch.long),\n",
        "                  train_mask=torch.tensor(train_mask,dtype=torch.bool),\n",
        "                  val_mask=torch.tensor(val_mask,dtype=torch.bool)) #, adj= torch.from_numpy(np.array(adj))\n",
        "\n",
        "  torch.manual_seed(42)\n",
        "\n",
        "  out =[]\n",
        "  for j in range(len(illicit_idx)):\n",
        "    node_idx = illicit_idx[j]\n",
        "    out.append(node_idx)\n",
        "  print(out)\n",
        "\n",
        "    #define a GCN class\n",
        "\n",
        "  class Net(torch.nn.Module):\n",
        "          def __init__(self, num_features, dim=16, num_classes=1):\n",
        "            super(Net, self).__init__()\n",
        "            self.conv1 = GCNConv(num_features, dim)\n",
        "            self.conv2 = GCNConv(dim, num_classes)\n",
        "\n",
        "          def forward(self, x, edge_index, data=None):\n",
        "            x = F.relu(self.conv1(x, edge_index))\n",
        "            x = F.dropout(x, training=self.training)\n",
        "            x = self.conv2(x, edge_index)\n",
        "            return F.log_softmax(x, dim=1)\n",
        "\n",
        "  epochs = 300\n",
        "  dim = 16\n",
        "\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  model = Net(num_features=data_train.num_node_features, dim=dim, num_classes=int(max(data_train.y).item())+1).to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-3)\n",
        "\n",
        "  def test(model, data):\n",
        "        model.eval()\n",
        "        logits, accs = model(data_train.x, data_train.edge_index, data), []\n",
        "        for _, mask in data('train_mask', 'val_mask'):\n",
        "            pred = logits[mask].max(1)[1]\n",
        "            acc = pred.eq(data_train.y[mask]).sum().item() / mask.sum().item()\n",
        "            accs.append(acc)\n",
        "        return accs\n",
        "\n",
        "\n",
        "  loss = 999.0\n",
        "  train_acc = 0.0\n",
        "  test_acc = 0.0\n",
        "\n",
        "  t = trange(epochs, desc=\"Stats: \", position=0)\n",
        "\n",
        "  for epoch in t:\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    data = data_train.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    log_logits = model(data_train.x, data_train.edge_index, data_train)\n",
        "\n",
        "      # Since the data is a single huge graph, training on the training set is done by masking the nodes that are not in the training set.\n",
        "    loss = F.nll_loss(log_logits[data_train.train_mask], data_train.y[data_train.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "      # validate\n",
        "    train_acc, test_acc = test(model, data_train)\n",
        "    train_loss = loss\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "    #Compute the coefficients of features\n",
        "  explainer = Explainer(\n",
        "      model=model,\n",
        "      algorithm=GNNExplainer(epochs=300),\n",
        "      explanation_type='model',\n",
        "      node_mask_type='attributes',\n",
        "      edge_mask_type='object',\n",
        "      model_config=dict(\n",
        "          mode='multiclass_classification',\n",
        "          task_level='node',\n",
        "          return_type='log_probs',\n",
        "  ),\n",
        ")\n",
        "\n",
        "  node_index = out\n",
        "  print(\"node_index:\", out)\n",
        "  explanation = explainer(data.x, data.edge_index, index=node_index)\n",
        "  print(f'Generated explanations in {explanation.available_explanations}')\n",
        "\n",
        "  #path = '/content/drive/MyDrive/GNNExplainer/TS_1/GCN/TS49_GCN/feature_importance_'+str(out[k])+'.png'\n",
        "  explanation.visualize_feature_importance(top_k=5)\n",
        "  #print(f\"Feature importance plot has been saved to '{path}'\")\n",
        "\n",
        "  path = '/content/drive/MyDrive/GNNExplainer/TS_1/GCN/subgraph_'+str(out[k])+'.png'\n",
        "  explanation.visualize_graph(path)\n",
        "\n",
        "#test() is called by passing the desired time step number as an argument.\n",
        "test(49)"
      ],
      "metadata": {
        "id": "-6TNWoew3Uea"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qLst8EMFmvqK",
        "MIT8Ra9N232r",
        "l5HYcSQ-mMRn",
        "-cuzcKRU3ObA"
      ],
      "authorship_tag": "ABX9TyNqbTs8HXIOy7HxJGNjj3GT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
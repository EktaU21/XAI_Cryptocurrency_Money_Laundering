{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EktaU21/XAI_Cryptocurrency_Money_Laundering/blob/main/GraphLIME.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSJ0LJ6kshit"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ladvGRRiLE3p"
      },
      "outputs": [],
      "source": [
        "! pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "! pip install torch-spline-conv -f https://data.pyg.org/whl/torch-{torch.__version__}.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpaUeW48tUYu"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Pm18hayxL_s5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.nn import Linear, LayerNorm, ReLU, Dropout\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import ChebConv, NNConv, DeepGCNLayer, GATConv, DenseGCNConv, GCNConv, GraphConv\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import scipy.sparse as sp\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHyYeDCEIGJr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import networkx as nx\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.utils import to_networkx\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "\n",
        "!pip install graphlime\n",
        "from graphlime import GraphLIME\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import ChebConv, NNConv, DeepGCNLayer, GATConv, DenseGCNConv, GCNConv, GraphConv\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm, trange"
      ],
      "metadata": {
        "id": "_KOca2fiK0AI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade graphlime\n"
      ],
      "metadata": {
        "id": "wNoc68ciDWtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/usr/local/lib/python3.11/dist-packages/graphlime/__init__.py\"\n",
        "\n",
        "# Read the contents of the file\n",
        "with open(file_path, \"r\") as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Remove all occurrences of \"normalize=False\" or \"normalize=True\"\n",
        "import re\n",
        "content = re.sub(r\",?\\s*normalize\\s*=\\s*(True|False)\", \"\", content)\n",
        "\n",
        "# Write the cleaned content back\n",
        "with open(file_path, \"w\") as file:\n",
        "    file.write(content)\n",
        "\n",
        "print(\"âœ… All 'normalize=' arguments removed from GraphLIME. Now restart your runtime and rerun the code.\")\n"
      ],
      "metadata": {
        "id": "CQqwbDdyDL2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GraphLIME_GAT**"
      ],
      "metadata": {
        "id": "5GPhGhGhKjWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(srno):\n",
        "\n",
        "  df_features = pd.read_csv('/content/drive/MyDrive/elliptic_bitcoin_dataset/elliptic_txs_features.csv', header=None)\n",
        "  df_edges = pd.read_csv('/content/drive/MyDrive/elliptic_bitcoin_dataset/elliptic_txs_edgelist.csv')\n",
        "  df_classes =  pd.read_csv('/content/drive/MyDrive/elliptic_bitcoin_dataset/elliptic_txs_classes.csv')\n",
        "  df_classes['class'] = df_classes['class'].map({'unknown': 2, '1':1, '2':0})\n",
        "\n",
        "  # merging dataframes\n",
        "  df_merge = df_features.merge(df_classes, how='left', right_on=\"txId\", left_on=0)\n",
        "  df_merge = df_merge.loc[df_merge[1] == srno]\n",
        "  df_merge = df_merge.sort_values(0).reset_index(drop=True)\n",
        "  classified = df_merge.loc[df_merge['class'].loc[df_merge['class']!=2].index].drop('txId', axis=1)\n",
        "  unclassified = df_merge.loc[df_merge['class'].loc[df_merge['class']==2].index].drop('txId', axis=1)\n",
        "\n",
        "  # storing classified unclassified nodes seperatly for training and testing purpose\n",
        "  classified_edges = df_edges.loc[df_edges['txId1'].isin(classified[0]) & df_edges['txId2'].isin(classified[0])]\n",
        "  unclassified_edges = df_edges.loc[df_edges['txId1'].isin(unclassified[0]) | df_edges['txId2'].isin(unclassified[0])]\n",
        "  del df_features, df_classes\n",
        "\n",
        "  edgess =  df_edges.loc[df_edges['txId1'].isin(df_merge[0]) & df_edges['txId2'].isin(df_merge[0])]\n",
        "\n",
        "  # all nodes in data\n",
        "  nodes = df_merge[0].values\n",
        "  map_id = {j:i for i,j in enumerate(nodes)} # mapping nodes to indexes\n",
        "\n",
        "  edges = edgess.copy()\n",
        "  edges.txId1 = edges.txId1.map(map_id)\n",
        "  edges.txId2 = edges.txId2.map(map_id)\n",
        "  #edges = edges.astype(int)\n",
        "\n",
        "  edge_index = np.array(edges.values).T\n",
        "\n",
        "  edge_index = torch.tensor(edge_index, dtype=torch.long).contiguous()\n",
        "  weights = torch.tensor([1]* edge_index.shape[1] , dtype=torch.double)\n",
        "  #print(edge_index.shape)\n",
        "\n",
        "  # maping txIds to corresponding indexes, to pass node features to the model\n",
        "  node_features = df_merge.drop(['txId'], axis=1).copy()\n",
        "  node_features[0] = node_features[0].map(map_id)\n",
        "  classified_idx = node_features['class'].loc[node_features['class']!=2].index\n",
        "  unclassified_idx = node_features['class'].loc[node_features['class']==2].index\n",
        "  # replace unkown class with 0, to avoid having 3 classes, this data/labels never used in training\n",
        "  node_features['class'] = node_features['class'].replace(2, 0)\n",
        "  #print(len(classified_idx))\n",
        "\n",
        "  labels = node_features['class'].values\n",
        "  node_features = torch.tensor(np.array(node_features.drop([0, 'class', 1], axis=1).values, dtype=np.double), dtype=torch.double)\n",
        "\n",
        "  # converting data to PyGeometric graph data format\n",
        "  data_train = Data(x=node_features, edge_index=edge_index, edge_attr=weights,\n",
        "                               y=torch.tensor(labels, dtype=torch.double)) #, adj= torch.from_numpy(np.array(adj))\n",
        "\n",
        "  #print(data_train)\n",
        "\n",
        "  y_train = labels[classified_idx]\n",
        "\n",
        "  # spliting train set and validation set\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  X_train, X_valid, y_train, y_valid, train_idx, valid_idx = train_test_split(node_features[classified_idx], y_train, classified_idx, test_size=0.15, random_state=42, stratify=y_train)\n",
        "\n",
        "  train_idx\n",
        "  ten_train_idx = torch.tensor(train_idx)\n",
        "  ten_train_idx\n",
        "\n",
        "  from torch_geometric.utils import index_to_mask, mask_to_index\n",
        "  train_mask = index_to_mask(ten_train_idx, size= len(labels))\n",
        "  train_mask.shape\n",
        "\n",
        "  valid_idx\n",
        "  ten_valid_idx = torch.tensor(valid_idx)\n",
        "  ten_valid_idx\n",
        "\n",
        "  from torch_geometric.utils import index_to_mask, mask_to_index\n",
        "  val_mask = index_to_mask(ten_valid_idx, size= len(labels))\n",
        "  val_mask.shape\n",
        "\n",
        "  # converting data to PyGeometric graph data format\n",
        "  data_train = Data(x=torch.tensor(node_features,dtype=torch.float),\n",
        "                  edge_index=edge_index, #edge_attr=weights,\n",
        "                               y=torch.tensor(labels, dtype=torch.long),\n",
        "                  train_mask=torch.tensor(train_mask,dtype=torch.bool),\n",
        "                  val_mask=torch.tensor(val_mask,dtype=torch.bool)) #, adj= torch.from_numpy(np.array(adj))\n",
        "\n",
        "  print(data_train)\n",
        "  with open(\"/content/drive/MyDrive/GraphLIME_TS/Information/result.txt\", \"a+\") as o:\n",
        "    o.write('\\nTime Step ' + str(srno) + '\\n')\n",
        "    o.write(f' {data_train}')\n",
        "\n",
        "  #visualize the Graph\n",
        "  plt.figure(figsize=(16, 12))\n",
        "  G = to_networkx(data_train, to_undirected=True)\n",
        "\n",
        "  #extract the degree value\n",
        "  degrees = dict(nx.degree(G))\n",
        "  node_indices = list(degrees.keys())\n",
        "  node_degrees = list(degrees.values())\n",
        "\n",
        "  max_degree = max(node_degrees)\n",
        "  node_idx = np.argmax(node_degrees).item()\n",
        "\n",
        "  plt.figure(figsize=(16, 4))\n",
        "  plt.bar(node_indices, node_degrees, width=5.0)\n",
        "  plt.vlines(x=node_idx, ymin=0, ymax=max_degree, colors='r')\n",
        "  plt.title('Degree of Nodes for Time Step'+str(srno))\n",
        "  plt.xlabel('Node Index')\n",
        "  plt.ylabel('Degree');\n",
        "\n",
        "  rootdir = '/content/drive/MyDrive/GraphLIME_TS/features/'\n",
        "  output_file = os.path.join(rootdir, 'TS'+ str(srno) + '.png')\n",
        "  plt.savefig(output_file)\n",
        "\n",
        "\n",
        "  print(f'Node {node_idx} has the largest degree value {max_degree}.')\n",
        "\n",
        "  #define a GAT class\n",
        "\n",
        "  class GAT(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim,\n",
        "                 heads_1=8, heads_2=1, att_dropout=0.6, input_dropout=0.6):\n",
        "\n",
        "        super(GAT, self).__init__()\n",
        "\n",
        "        self.att_dropout = att_dropout\n",
        "        self.input_dropout = input_dropout\n",
        "\n",
        "        self.conv1 = GATConv(in_channels=input_dim,\n",
        "                             out_channels=hidden_dim // heads_1,\n",
        "                             heads=heads_1,\n",
        "                             concat=True,\n",
        "                             dropout=att_dropout)\n",
        "        self.conv2 = GATConv(in_channels=hidden_dim,\n",
        "                             out_channels=output_dim,\n",
        "                             heads=heads_2,\n",
        "                             concat=False,\n",
        "                             dropout=att_dropout)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.dropout(x, p=self.input_dropout, training=self.training)\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=self.input_dropout, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        " #Instantiate a GAT model\n",
        "  hparams = {\n",
        "    'input_dim': data_train.num_node_features,\n",
        "    'hidden_dim':8,\n",
        "    'output_dim': int(max(data_train.y).item()) + 1\n",
        "  }\n",
        "  model = GAT(**hparams)\n",
        "  model\n",
        "\n",
        "  #Train the model\n",
        "  def accuracy(output, labels):\n",
        "    _, pred = output.max(dim=1)\n",
        "    correct = pred.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "\n",
        "    return correct / len(labels)\n",
        "\n",
        "  lr = 0.005\n",
        "  #change the lr and epochs to 0.01 and 200\n",
        "  epochs =300\n",
        "\n",
        "  model.train()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = model(data_train.x, data_train.edge_index)\n",
        "    loss = F.nll_loss(output[data_train.train_mask], data_train.y[data_train.train_mask])\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        acc = accuracy(output[data_train.train_mask], data_train.y[data_train.train_mask])\n",
        "        print('Epoch: {:3d}, acc = {:.3f}'.format(epoch, acc))\n",
        "\n",
        "  #Explain node features\n",
        "  node_idx\n",
        "  model.eval()\n",
        "\n",
        "  # instantiate a GraphLIME object\n",
        "  explainer = GraphLIME(model, hop=2, rho=0.1, cached=True)\n",
        "\n",
        "  # explain node features by calling the method `explain_node()`\n",
        "  coefs = explainer.explain_node(node_idx, data_train.x, data_train.edge_index)\n",
        "  coefs\n",
        "\n",
        "  with open(\"/content/drive/MyDrive/GraphLIME_TS/Information/matrix.csv\", \"a+\") as o:\n",
        "    o.write(\"Coefficient of Time Step\"+str(srno)+'\\n')\n",
        "    o.write(f' {coefs}')\n",
        "    o.write(\"\\n\")\n",
        "\n",
        "\n",
        "  #visualize the coefficeints\n",
        "  plt.figure(figsize=(16, 4))\n",
        "\n",
        "  x = list(range(data_train.num_node_features))\n",
        "\n",
        "  plt.bar(x, coefs, width=5.0)\n",
        "  plt.xlabel('Feature Index')\n",
        "  plt.ylabel(r'$\\beta$');\n",
        "  out = \"Time Step\" + str(1);\n",
        "  plt.title(out);\n",
        "\n",
        "  print(f'The {np.argmax(coefs)}-th feature is the most important.')\n",
        "\n",
        "test(3)"
      ],
      "metadata": {
        "id": "sXu4XKCWBABj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **GraphLIME_GCN**"
      ],
      "metadata": {
        "id": "bV2B6qzELKuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(srno):\n",
        "\n",
        "  df_features = pd.read_csv('/content/drive/MyDrive/elliptic_bitcoin_dataset/elliptic_txs_features.csv', header=None)\n",
        "  df_edges = pd.read_csv('/content/drive/MyDrive/elliptic_bitcoin_dataset/elliptic_txs_edgelist.csv')\n",
        "  df_classes =  pd.read_csv('/content/drive/MyDrive/elliptic_bitcoin_dataset/elliptic_txs_classes.csv')\n",
        "  df_classes['class'] = df_classes['class'].map({'unknown': 2, '1':1, '2':0})\n",
        "\n",
        "  # merging dataframes\n",
        "  df_merge = df_features.merge(df_classes, how='left', right_on=\"txId\", left_on=0)\n",
        "  df_merge = df_merge.loc[df_merge[1] == srno]\n",
        "  df_merge = df_merge.sort_values(0).reset_index(drop=True)\n",
        "  classified = df_merge.loc[df_merge['class'].loc[df_merge['class']!=2].index].drop('txId', axis=1)\n",
        "  unclassified = df_merge.loc[df_merge['class'].loc[df_merge['class']==2].index].drop('txId', axis=1)\n",
        "\n",
        "  # storing classified unclassified nodes seperatly for training and testing purpose\n",
        "  classified_edges = df_edges.loc[df_edges['txId1'].isin(classified[0]) & df_edges['txId2'].isin(classified[0])]\n",
        "  unclassified_edges = df_edges.loc[df_edges['txId1'].isin(unclassified[0]) | df_edges['txId2'].isin(unclassified[0])]\n",
        "  del df_features, df_classes\n",
        "\n",
        "  edgess =  df_edges.loc[df_edges['txId1'].isin(df_merge[0]) & df_edges['txId2'].isin(df_merge[0])]\n",
        "\n",
        "  # all nodes in data\n",
        "  nodes = df_merge[0].values\n",
        "  map_id = {j:i for i,j in enumerate(nodes)} # mapping nodes to indexes\n",
        "\n",
        "  edges = edgess.copy()\n",
        "  edges.txId1 = edges.txId1.map(map_id)\n",
        "  edges.txId2 = edges.txId2.map(map_id)\n",
        "\n",
        "  edge_index = np.array(edges.values).T\n",
        "  edge_index = torch.tensor(edge_index, dtype=torch.long).contiguous()\n",
        "  weights = torch.tensor([1]* edge_index.shape[1] , dtype=torch.double)\n",
        "\n",
        "  # maping txIds to corresponding indexes, to pass node features to the model\n",
        "  node_features = df_merge.drop(['txId'], axis=1).copy()\n",
        "  node_features[0] = node_features[0].map(map_id)\n",
        "  classified_idx = node_features['class'].loc[node_features['class']!=2].index\n",
        "  unclassified_idx = node_features['class'].loc[node_features['class']==2].index\n",
        "  # replace unkown class with 0, to avoid having 3 classes, this data/labels never used in training\n",
        "  node_features['class'] = node_features['class'].replace(2, 0)\n",
        "\n",
        "  labels = node_features['class'].values\n",
        "  node_features = torch.tensor(np.array(node_features.drop([0, 'class', 1], axis=1).values, dtype=np.double), dtype=torch.double)\n",
        "\n",
        "  # converting data to PyGeometric graph data format\n",
        "  data_train = Data(x=node_features, edge_index=edge_index, edge_attr=weights,\n",
        "                               y=torch.tensor(labels, dtype=torch.double)) #, adj= torch.from_numpy(np.array(adj))\n",
        "\n",
        "\n",
        "  y_train = labels[classified_idx]\n",
        "\n",
        "  # spliting train set and validation set\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  X_train, X_valid, y_train, y_valid, train_idx, valid_idx = train_test_split(node_features[classified_idx], y_train, classified_idx, test_size=0.15, random_state=42, stratify=y_train)\n",
        "\n",
        "  train_idx\n",
        "  ten_train_idx = torch.tensor(train_idx)\n",
        "  ten_train_idx\n",
        "\n",
        "  from torch_geometric.utils import index_to_mask, mask_to_index\n",
        "  train_mask = index_to_mask(ten_train_idx, size= len(labels))\n",
        "  train_mask.shape\n",
        "\n",
        "  valid_idx\n",
        "  ten_valid_idx = torch.tensor(valid_idx)\n",
        "  ten_valid_idx\n",
        "\n",
        "  from torch_geometric.utils import index_to_mask, mask_to_index\n",
        "  val_mask = index_to_mask(ten_valid_idx, size= len(labels))\n",
        "  val_mask.shape\n",
        "\n",
        "  # converting data to PyGeometric graph data format\n",
        "  data_train = Data(x=torch.tensor(node_features,dtype=torch.float),\n",
        "                  edge_index=edge_index, #edge_attr=weights,\n",
        "                               y=torch.tensor(labels, dtype=torch.long),\n",
        "                  train_mask=torch.tensor(train_mask,dtype=torch.bool),\n",
        "                  val_mask=torch.tensor(val_mask,dtype=torch.bool)) #, adj= torch.from_numpy(np.array(adj))\n",
        "\n",
        "  print(data_train)\n",
        "  #outt_file = '/content/drive/MyDrive/GraphLIME_TS/Information/result'+ str(srno) + '.txt'\n",
        "  with open(\"/content/drive/MyDrive/GraphLIME_TS/Information/result.txt\", \"a+\") as o:\n",
        "    o.write('\\nTime Step ' + str(srno) + '\\n')\n",
        "    o.write(f' {data_train}')\n",
        "\n",
        "  #visualize the Graph\n",
        "  plt.figure(figsize=(16, 12))\n",
        "  G = to_networkx(data_train, to_undirected=True)\n",
        "  #pos = nx.spring_layout(G, k=0.1)\n",
        "  #nx.draw_networkx(G, with_labels=False, node_size=30)\n",
        "  #plt.title('AML');\n",
        "\n",
        "  #extract the degree value\n",
        "  degrees = dict(nx.degree(G))\n",
        "  node_indices = list(degrees.keys())\n",
        "  node_degrees = list(degrees.values())\n",
        "\n",
        "  max_degree = max(node_degrees)\n",
        "  node_idx = np.argmax(node_degrees).item()\n",
        "\n",
        "  plt.figure(figsize=(16, 4))\n",
        "  plt.bar(node_indices, node_degrees, width=5.0)\n",
        "  plt.vlines(x=node_idx, ymin=0, ymax=max_degree, colors='r')\n",
        "  plt.title('Degree of Nodes for Time Step'+str(srno))\n",
        "  plt.xlabel('Node Index')\n",
        "  plt.ylabel('Degree');\n",
        "\n",
        "  rootdir = '/content/drive/MyDrive/GraphLIME_TS/features/'\n",
        "  output_file = os.path.join(rootdir, 'TS'+ str(srno) + '.png')\n",
        "  plt.savefig(output_file)\n",
        "\n",
        "\n",
        "  print(f'Node {node_idx} has the largest degree value {max_degree}.')\n",
        "\n",
        "  #define a GCN class\n",
        "\n",
        "  class Net(torch.nn.Module):\n",
        "          def __init__(self, num_features, dim=16, num_classes=1):\n",
        "            super(Net, self).__init__()\n",
        "            self.conv1 = GCNConv(num_features, dim)\n",
        "            self.conv2 = GCNConv(dim, num_classes)\n",
        "\n",
        "          def forward(self, x, edge_index, data=None):\n",
        "            x = F.relu(self.conv1(x, edge_index))\n",
        "            x = F.dropout(x, training=self.training)\n",
        "            x = self.conv2(x, edge_index)\n",
        "            return F.log_softmax(x, dim=1)\n",
        "\n",
        "  epochs = 300\n",
        "  dim = 16\n",
        "\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  model = Net(num_features=data_train.num_node_features, dim=dim, num_classes=int(max(data_train.y).item())+1).to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-3)\n",
        "\n",
        "  def test(model, data):\n",
        "        model.eval()\n",
        "        logits, accs = model(data_train.x, data_train.edge_index, data), []\n",
        "        for _, mask in data('train_mask', 'val_mask'):\n",
        "            pred = logits[mask].max(1)[1]\n",
        "            acc = pred.eq(data_train.y[mask]).sum().item() / mask.sum().item()\n",
        "            accs.append(acc)\n",
        "        return accs\n",
        "\n",
        "\n",
        "  loss = 999.0\n",
        "  train_acc = 0.0\n",
        "  test_acc = 0.0\n",
        "\n",
        "  t = trange(epochs, desc=\"Stats: \", position=0)\n",
        "\n",
        "  for epoch in t:\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    data = data_train.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    log_logits = model(data_train.x, data_train.edge_index, data_train)\n",
        "\n",
        "     # Since the data is a single huge graph, training on the training set is done by masking the nodes that are not in the training set.\n",
        "    loss = F.nll_loss(log_logits[data_train.train_mask], data_train.y[data_train.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # validate\n",
        "    train_acc, test_acc = test(model, data_train)\n",
        "    train_loss = loss\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "  # instantiate a GraphLIME object\n",
        "  explainer = GraphLIME(model, hop=2, rho=0.1, cached=True)\n",
        "\n",
        "  # explain node features by calling the method `explain_node()`\n",
        "  coefs = explainer.explain_node(node_idx, data_train.x, data_train.edge_index)\n",
        "  coefs\n",
        "\n",
        "  with open(\"/content/drive/MyDrive/GraphLIME_TS/Information/matrix.csv\", \"a+\") as o:\n",
        "    o.write(\"Coefficient of Time Step\"+str(srno)+'\\n')\n",
        "    o.write(f' {coefs}')\n",
        "    o.write(\"\\n\")\n",
        "\n",
        "\n",
        "  #visualize the coefficeints\n",
        "  plt.figure(figsize=(16, 4))\n",
        "\n",
        "  x = list(range(data_train.num_node_features))\n",
        "\n",
        "  plt.bar(x, coefs, width=5.0)\n",
        "  plt.xlabel('Feature Index')\n",
        "  plt.ylabel(r'$\\beta$');\n",
        "  out = \"Time Step\" + str(1);\n",
        "  plt.title(out);\n",
        "\n",
        "  print(f'The {np.argmax(coefs)}-th feature is the most important.')\n",
        "\n",
        "test(3)"
      ],
      "metadata": {
        "id": "m_VNEeUxH1VR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
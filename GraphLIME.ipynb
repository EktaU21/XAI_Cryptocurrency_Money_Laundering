{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EktaU21/XAI_Cryptocurrency_Money_Laundering/blob/main/GraphLIME.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSJ0LJ6kshit"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ladvGRRiLE3p"
      },
      "outputs": [],
      "source": [
        "! pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "! pip install torch-spline-conv -f https://data.pyg.org/whl/torch-{torch.__version__}.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpaUeW48tUYu",
        "outputId": "2c95b4f5-6a97-4ea8-e66b-6cd200c12cd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pm18hayxL_s5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.nn import Linear, LayerNorm, ReLU, Dropout\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import ChebConv, NNConv, DeepGCNLayer, GATConv, DenseGCNConv, GCNConv, GraphConv\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import scipy.sparse as sp\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHyYeDCEIGJr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import networkx as nx\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "#import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#print(torch.__version__)\n",
        "#!pip install torch-geometric\n",
        "\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.utils import to_networkx\n",
        "#from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "\n",
        "!pip install graphlime\n",
        "from graphlime import GraphLIME\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(srno):\n",
        "\n",
        "  df_features = pd.read_csv('/content/drive/MyDrive/elliptic_bitcoin_dataset/elliptic_txs_features.csv', header=None)\n",
        "  df_edges = pd.read_csv('/content/drive/MyDrive/elliptic_bitcoin_dataset/elliptic_txs_edgelist.csv')\n",
        "  df_classes =  pd.read_csv('/content/drive/MyDrive/elliptic_bitcoin_dataset/elliptic_txs_classes.csv')\n",
        "  df_classes['class'] = df_classes['class'].map({'unknown': 2, '1':1, '2':0})\n",
        "\n",
        "  # merging dataframes\n",
        "  df_merge = df_features.merge(df_classes, how='left', right_on=\"txId\", left_on=0)\n",
        "  df_merge = df_merge.loc[df_merge[1] == srno]\n",
        "  df_merge = df_merge.sort_values(0).reset_index(drop=True)\n",
        "  classified = df_merge.loc[df_merge['class'].loc[df_merge['class']!=2].index].drop('txId', axis=1)\n",
        "  unclassified = df_merge.loc[df_merge['class'].loc[df_merge['class']==2].index].drop('txId', axis=1)\n",
        "\n",
        "  # storing classified unclassified nodes seperatly for training and testing purpose\n",
        "  classified_edges = df_edges.loc[df_edges['txId1'].isin(classified[0]) & df_edges['txId2'].isin(classified[0])]\n",
        "  unclassified_edges = df_edges.loc[df_edges['txId1'].isin(unclassified[0]) | df_edges['txId2'].isin(unclassified[0])]\n",
        "  del df_features, df_classes\n",
        "\n",
        "  edgess =  df_edges.loc[df_edges['txId1'].isin(df_merge[0]) & df_edges['txId2'].isin(df_merge[0])]\n",
        "\n",
        "  # all nodes in data\n",
        "  nodes = df_merge[0].values\n",
        "  map_id = {j:i for i,j in enumerate(nodes)} # mapping nodes to indexes\n",
        "\n",
        "  edges = edgess.copy()\n",
        "  edges.txId1 = edges.txId1.map(map_id)\n",
        "  edges.txId2 = edges.txId2.map(map_id)\n",
        "\n",
        "  edge_index = np.array(edges.values).T\n",
        "\n",
        "  edge_index = torch.tensor(edge_index, dtype=torch.long).contiguous()\n",
        "  weights = torch.tensor([1]* edge_index.shape[1] , dtype=torch.double)\n",
        "\n",
        "  # maping txIds to corresponding indexes, to pass node features to the model\n",
        "  node_features = df_merge.drop(['txId'], axis=1).copy()\n",
        "  node_features[0] = node_features[0].map(map_id)\n",
        "  classified_idx = node_features['class'].loc[node_features['class']!=2].index\n",
        "  unclassified_idx = node_features['class'].loc[node_features['class']==2].index\n",
        "  # replace unkown class with 0, to avoid having 3 classes, this data/labels never used in training\n",
        "  node_features['class'] = node_features['class'].replace(2, 0)\n",
        "\n",
        "  labels = node_features['class'].values\n",
        "  node_features = torch.tensor(np.array(node_features.drop([0, 'class', 1], axis=1).values, dtype=np.double), dtype=torch.double)\n",
        "\n",
        "  # converting data to PyGeometric graph data format\n",
        "  data_train = Data(x=node_features, edge_index=edge_index, edge_attr=weights,\n",
        "                               y=torch.tensor(labels, dtype=torch.double)) #, adj= torch.from_numpy(np.array(adj))\n",
        "\n",
        "  y_train = labels[classified_idx]\n",
        "\n",
        "  # spliting train set and validation set\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  X_train, X_valid, y_train, y_valid, train_idx, valid_idx = train_test_split(node_features[classified_idx], y_train, classified_idx, test_size=0.15, random_state=42, stratify=y_train)\n",
        "\n",
        "  train_idx\n",
        "  ten_train_idx = torch.tensor(train_idx)\n",
        "  ten_train_idx\n",
        "\n",
        "  from torch_geometric.utils import index_to_mask, mask_to_index\n",
        "  train_mask = index_to_mask(ten_train_idx, size= len(labels))\n",
        "  train_mask.shape\n",
        "\n",
        "  valid_idx\n",
        "  ten_valid_idx = torch.tensor(valid_idx)\n",
        "  ten_valid_idx\n",
        "\n",
        "  from torch_geometric.utils import index_to_mask, mask_to_index\n",
        "  val_mask = index_to_mask(ten_valid_idx, size= len(labels))\n",
        "  val_mask.shape\n",
        "\n",
        "  # converting data to PyGeometric graph data format\n",
        "  data_train = Data(x=torch.tensor(node_features,dtype=torch.float),\n",
        "                  edge_index=edge_index, #edge_attr=weights,\n",
        "                               y=torch.tensor(labels, dtype=torch.long),\n",
        "                  train_mask=torch.tensor(train_mask,dtype=torch.bool),\n",
        "                  val_mask=torch.tensor(val_mask,dtype=torch.bool)) #, adj= torch.from_numpy(np.array(adj))\n",
        "\n",
        "  print(data_train)\n",
        "  #outt_file = '/content/drive/MyDrive/GraphLIME_TS/Information/result'+ str(srno) + '.txt'\n",
        "  with open(\"/content/drive/MyDrive/GraphLIME_TS/Information/result.txt\", \"a+\") as o:\n",
        "    o.write('\\nTime Step ' + str(srno) + '\\n')\n",
        "    o.write(f' {data_train}')\n",
        "\n",
        "  #visualize the Graph\n",
        "  plt.figure(figsize=(16, 12))\n",
        "  G = to_networkx(data_train, to_undirected=True)\n",
        "\n",
        "  #extract the degree value\n",
        "  degrees = dict(nx.degree(G))\n",
        "  node_indices = list(degrees.keys())\n",
        "  node_degrees = list(degrees.values())\n",
        "\n",
        "  max_degree = max(node_degrees)\n",
        "  node_idx = np.argmax(node_degrees).item()\n",
        "\n",
        "  plt.figure(figsize=(16, 4))\n",
        "  plt.bar(node_indices, node_degrees, width=5.0)\n",
        "  plt.vlines(x=node_idx, ymin=0, ymax=max_degree, colors='r')\n",
        "  plt.title('Degree of Nodes for Time Step'+str(srno))\n",
        "  plt.xlabel('Node Index')\n",
        "  plt.ylabel('Degree');\n",
        "\n",
        "  rootdir = '/content/drive/MyDrive/GraphLIME_TS/Node_degree/'\n",
        "  plt.rcParams[\"savefig.directory\"] = os.chdir(os.path.dirname(rootdir))\n",
        "  out_file = 'Node_TS'+ str(srno) + '.png'\n",
        "  plt.savefig(out_file)\n",
        "  plt.close()\n",
        "\n",
        "  with open(\"/content/drive/MyDrive/GraphLIME_TS/Information/result.txt\", \"a+\") as o:\n",
        "    o.write(\"\\n\")\n",
        "    o.write(f'Node {node_idx} has the largest degree value {max_degree}.')\n",
        "\n",
        "  #define a GAT class\n",
        "\n",
        "  class GAT(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim,\n",
        "                 heads_1=8, heads_2=1, att_dropout=0.6, input_dropout=0.6):\n",
        "\n",
        "        super(GAT, self).__init__()\n",
        "\n",
        "        self.att_dropout = att_dropout\n",
        "        self.input_dropout = input_dropout\n",
        "\n",
        "        self.conv1 = GATConv(in_channels=input_dim,\n",
        "                             out_channels=hidden_dim // heads_1,\n",
        "                             heads=heads_1,\n",
        "                             concat=True,\n",
        "                             dropout=att_dropout)\n",
        "        self.conv2 = GATConv(in_channels=hidden_dim,\n",
        "                             out_channels=output_dim,\n",
        "                             heads=heads_2,\n",
        "                             concat=False,\n",
        "                             dropout=att_dropout)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.dropout(x, p=self.input_dropout, training=self.training)\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=self.input_dropout, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        " #Instantiate a GAT model\n",
        "  hparams = {\n",
        "    'input_dim': data_train.num_node_features,\n",
        "    'hidden_dim':8,\n",
        "    'output_dim': int(max(data_train.y).item()) + 1\n",
        "  }\n",
        "  model = GAT(**hparams)\n",
        "  model\n",
        "\n",
        "  #Train the model\n",
        "  def accuracy(output, labels):\n",
        "    _, pred = output.max(dim=1)\n",
        "    correct = pred.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "\n",
        "    return correct / len(labels)\n",
        "\n",
        "  lr = 0.005\n",
        "  #change the lr and epochs to 0.01 and 200\n",
        "  epochs =300\n",
        "\n",
        "  model.train()\n",
        "  optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = model(data_train.x, data_train.edge_index)\n",
        "    loss = F.nll_loss(output[data_train.train_mask], data_train.y[data_train.train_mask])\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        acc = accuracy(output[data_train.train_mask], data_train.y[data_train.train_mask])\n",
        "        #print('Epoch: {:3d}, acc = {:.3f}'.format(epoch, acc))\n",
        "        with open(\"/content/drive/MyDrive/GraphLIME_TS/Information/result.txt\", \"a+\") as o:\n",
        "          o.write(\"\\n\")\n",
        "          o.writelines('Epoch: {:3d}, acc = {:.3f}'.format(epoch, acc))\n",
        "          o.write(\"\\n\")\n",
        "\n",
        "  #Explain node features\n",
        "  node_idx\n",
        "  model.eval()\n",
        "\n",
        "  #Compute the coefficients of features\n",
        "\n",
        "  # instantiate a GraphLIME object\n",
        "  explainer = GraphLIME(model, hop=2, rho=0.1, cached=True)\n",
        "\n",
        "  # explain node features by calling the method `explain_node()`\n",
        "  coefs = explainer.explain_node(node_idx, data_train.x, data_train.edge_index)\n",
        "  coefs\n",
        "\n",
        "  with open(\"/content/drive/MyDrive/GraphLIME_TS/Information/matrix.csv\", \"a+\") as o:\n",
        "    o.write(\"Coefficient of Time Step\"+str(srno)+'\\n')\n",
        "    o.write(f' {coefs}')\n",
        "    o.write(\"\\n\")\n",
        "\n",
        "  #visualize the coefficeints\n",
        "\n",
        "  plt.figure(figsize=(16, 4))\n",
        "\n",
        "  x = list(range(data_train.num_node_features))\n",
        "\n",
        "  plt.bar(x, coefs, width=5.0)\n",
        "  plt.xlabel('Feature Index')\n",
        "  plt.ylabel(r'$\\beta$');\n",
        "  plt.title(\"Time Step\"+str(srno));\n",
        "\n",
        "  rootdir = '/content/drive/MyDrive/GraphLIME_TS/features/'\n",
        "  plt.rcParams[\"savefig.directory\"] = os.chdir(os.path.dirname(rootdir))\n",
        "  output_file = 'TS'+ str(srno) + '.png'\n",
        "  plt.savefig(output_file)\n",
        "  plt.close()\n",
        "\n",
        "  print(f'The {np.argmax(coefs)}-th feature is the most important.')\n",
        "  with open(\"/content/drive/MyDrive/GraphLIME_TS/Information/result.txt\", \"a+\") as o:\n",
        "          o.write(\"\\n\")\n",
        "          o.write(f'The {np.argmax(coefs)}-th feature is the most important.')\n",
        "\n",
        "#test() is called by passing the desired time step number as an argument.\n",
        "for i in range(1,50):\n",
        "  test(i)"
      ],
      "metadata": {
        "id": "A4Ez9TD0c3cB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}